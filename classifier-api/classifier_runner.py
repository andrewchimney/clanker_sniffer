# classifier_runner.py
import os
import json
import logging
from typing import Optional, Dict, Any
from fastapi import FastAPI, HTTPException, Body, Form
from pydantic import BaseModel
import httpx

# ----------------------------
# Logging
# ----------------------------
logging.basicConfig(level=logging.INFO, format="%(levelname)-9s %(message)s")
logger = logging.getLogger("classify")

# ----------------------------
# Env config
# ----------------------------
# AI_PROVIDER: "ollama" (default) or "openai" (LocalAI / OpenAI-compatible)
AI_PROVIDER = os.getenv("AI_PROVIDER", "ollama").strip().lower()

# Base URLs:
# - For Ollama (docker compose service name): http://ollama:11434
# - For LocalAI/OpenAI-compatible: http://localai:8080/v1  (must end with /v1)
AI_BASE_URL = os.getenv(
    "AI_BASE_URL",
    "http://ollama:11434" if AI_PROVIDER == "ollama" else "http://localai:8080/v1",
).rstrip("/")

# Default models
AI_MODEL = os.getenv(
    "AI_MODEL",
    "qwen2.5:3b-instruct" if AI_PROVIDER == "ollama" else "mistral:instruct",
)

# Optional API key for OpenAI-compatible servers (ignored by Ollama)
AI_API_KEY = os.getenv("AI_API_KEY", "").strip()

# Generation settings
AI_TEMPERATURE = float(os.getenv("AI_TEMPERATURE", "0"))
AI_NUM_CTX = int(os.getenv("AI_NUM_CTX", "4096"))
AI_TIMEOUT_SECS = float(os.getenv("AI_TIMEOUT_SECS", "120"))

# ----------------------------
# Prompts
# ----------------------------
SYS = (
    "You are a careful content-provenance judge. "
    "Given song LYRICS, estimate if they were generated by an AI model. "
    "Return STRICT JSON only."
)

USER_TMPL = """LYRICS:
{lyrics}

Return STRICT JSON with keys:
- ai_probability: float in [0,1]
- label: one of ["AI-likely","Human","Uncertain"]
- rationale: <=2 short sentences (surface cues only)
No extra text or markdown fences â€” JSON only.
"""

# ----------------------------
# FastAPI app
# ----------------------------
app = FastAPI()


class LyricsInput(BaseModel):
    lyrics: str


@app.get("/health")
async def health():
    return {"status": "ok"}


@app.get("/health/llm")
async def llm_health():
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            if AI_PROVIDER == "ollama":
                r = await client.get(f"{AI_BASE_URL}/api/tags")
                ok = r.status_code == 200
                body = r.json() if ok else {"status_code": r.status_code}
                return {"ok": ok, "provider": "ollama", "models": body}
            else:
                # OpenAI/LocalAI-compatible
                headers = _auth_header()
                r = await client.get(f"{AI_BASE_URL}/models", headers=headers)
                ok = r.status_code == 200
                body = r.json() if ok else {"status_code": r.status_code}
                return {"ok": ok, "provider": "openai", "models": body}
    except Exception as e:
        return {"ok": False, "provider": AI_PROVIDER, "error": str(e), "base_url": AI_BASE_URL}


def _auth_header() -> Dict[str, str]:
    return {"Authorization": f"Bearer {AI_API_KEY}"} if (AI_PROVIDER != "ollama" and AI_API_KEY) else {}


# ----------------------------
# LLM calls
# ----------------------------
async def _ask_llm(lyrics: str) -> Dict[str, Any]:
    """
    Ask the model to return a JSON object containing:
      { "ai_probability": float, "label": "AI-likely"|"Human"|"Uncertain", "rationale": "..."}
    """
    if not lyrics or not lyrics.strip():
        raise HTTPException(status_code=400, detail="Empty lyrics")

    sys_msg = {"role": "system", "content": SYS}
    user_msg = {"role": "user", "content": USER_TMPL.format(lyrics=lyrics)}

    if AI_PROVIDER == "ollama":
        payload = {
            "model": AI_MODEL,
            "stream": False,
            "messages": [sys_msg, user_msg],
            "options": {"temperature": AI_TEMPERATURE, "num_ctx": AI_NUM_CTX},
        }
        async with httpx.AsyncClient(timeout=AI_TIMEOUT_SECS) as client:
            res = await client.post(f"{AI_BASE_URL}/api/chat", json=payload)
            try:
                res.raise_for_status()
            except httpx.HTTPStatusError as e:
                logger.error("Ollama error: %s | body=%s", e, getattr(e.response, "text", "")[:500])
                logger.info(e)
                raise HTTPException(status_code=502, detail="Model server error (ollama)")
            j = res.json()
            content = (j.get("message") or {}).get("content", "").strip()
    # else:
    #     # OpenAI/LocalAI-compatible
    #     payload = {
    #         "model": AI_MODEL,
    #         "temperature": AI_TEMPERATURE,
    #         "response_format": {"type": "json_object"},
    #         "messages": [sys_msg, user_msg],
    #     }
    #     async with httpx.AsyncClient(timeout=AI_TIMEOUT_SECS) as client:
    #         res = await client.post(
    #             f"{AI_BASE_URL}/chat/completions",
    #             headers=_auth_header(),
    #             json=payload,
    #         )
    #         try:
    #             res.raise_for_status()
    #         except httpx.HTTPStatusError as e:
    #             logger.error("OpenAI-compatible error: %s | body=%s", e, getattr(e.response, "text", "")[:500])
    #             raise HTTPException(status_code=502, detail="Model server error (openai-compatible)")
    #         j = res.json()
    #         content = j["choices"][0]["message"]["content"].strip()

    # Parse the model JSON (with code-fence fallback)
    data = _parse_json_loose(content)

    # Sanitize fields
    p = data.get("ai_probability")
    try:
        p = float(p) if p is not None else None
    except Exception:
        p = None
    if p is not None:
        p = max(0.0, min(1.0, p))

    label = (data.get("label") or "").strip()
    return {"ai_probability": p, "label": label}


def _parse_json_loose(s: str) -> Dict[str, Any]:
    try:
        return json.loads(s)
    except Exception:
        pass
    if "```" in s:
        parts = s.split("```")
        # try the first JSON-looking chunk from the back
        for chunk in reversed(parts):
            chunk = chunk.strip()
            if chunk.startswith("{") and chunk.endswith("}"):
                try:
                    return json.loads(chunk)
                except Exception:
                    continue
    # last resort: extract a substring that looks like JSON object
    start = s.find("{")
    end = s.rfind("}")
    if start != -1 and end != -1 and end > start:
        try:
            return json.loads(s[start : end + 1])
        except Exception:
            return {}
    return {}


def _collapse_to_two_fields(judge: Dict[str, Any]) -> Dict[str, Any]:
    """
    Reduce judge output to exactly:
      { "classification": "AI"|"Human", "accuracy": float }
    """
    p = judge.get("ai_probability")
    try:
        p = float(p) if p is not None else None
    except Exception:
        p = None
    if p is not None:
        p = max(0.0, min(1.0, p))

    raw = (judge.get("label") or "").lower()
    if raw.startswith("ai"):
        classification = "AI"
    elif raw == "human":
        classification = "Human"
    else:
        # fallback by probability or default to 0.5 if missing
        if p is None:
            p = 0.5
        classification = "AI" if p > 0.5 else "Human"

    accuracy = p if classification == "AI" else (1.0 - p)
    return {"classification": classification, "accuracy": round(float(accuracy), 4)}


# ----------------------------
# Endpoint
# ----------------------------
@app.post("/classify")
async def classify(lyrics: str = Form(...)):
    logger.info(lyrics)
    if not lyrics or not lyrics.strip():
        raise HTTPException(400, detail="Missing 'lyrics'")

    logger.info("ðŸŸ¦Classifying Lyrics (%s) model=%s", AI_PROVIDER, AI_MODEL)
    try:
        judge = await _ask_llm(lyrics)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("LLM call failed")
        raise HTTPException(status_code=500, detail=str(e))

    out = _collapse_to_two_fields(judge)
    logger.info("ðŸŸ¦Result: %s (accuracy=%s)", out["classification"], out["accuracy"])
    return out
